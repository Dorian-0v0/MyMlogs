## 爬虫的基本步骤

1. **明确目标**：确定要爬取的网站或网页，明确需要提取的数据内容。
2. **发送请求**：使用如`requests`库向目标网页发起请求，获取网页HTML内容。
3. **解析网页**：用`BeautifulSoup`、`lxml`或`re`等工具解析网页内容，提取所需信息。
4. **存储数据**：将提取的数据保存到本地文件（CSV、JSON、数据库等）。

#### 遵守规则

1. 不要爬取国家事务，国防安全等相关数据
2. 不要爬取隐私数据
3. 请求次数不要过高，否则会变为Ddos攻击
4. 网站反爬就不要强行去爬
5. 尊重目标网站的robots.txt规则和版权，不爬取敏感或受保护内容。（比如b站：[bilibili.com/robots.txt](https://www.bilibili.com/robots.txt)）

## 具体步骤

1. #### 安装requests库

   ```
   pip install requests
   ```

2. #### 代码结构

   ```python
   import requests
   head = { "User-Agent": "Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Mobile Safari/537.36 Edg/136.0.0.0" } # 篡改请求头，模拟浏览器发送请求
   response = requests.get("http://books.toscrape.com/", headers = head) # 模拟发送get请求
   if response.ok:
       print(response.text)
   else:
       print("请求失败")
       pass
       #......
       # 之后就是怎么具体地处理数据
   ```

   